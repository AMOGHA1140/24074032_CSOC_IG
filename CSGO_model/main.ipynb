{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3339e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2, os, json\n",
    "import time, random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0370b8e",
   "metadata": {},
   "source": [
    "#### 1. Creating the clips\n",
    "\n",
    "* Firstly, the timestamps are manually marked and kept in a text-file, with a corresponding url of the yt video. The video is then downloaded locally.\n",
    "* The downloaded video is loaded, then a clip of 3 seconds, 2s before the timestamp and 1s after, is created and stored in `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc93de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "from pytubefix.cli import on_progress\n",
    "\n",
    "\n",
    "# download the yt videos\n",
    "\n",
    "video1_url = 'www.youtube.com/watch?v=P22HqM9w500'\n",
    "video2_url = \"https://www.youtube.com/watch?v=GsLhzD72yYA\"\n",
    "video3_url = 'https://www.youtube.com/watch?v=k4uLbGkm6Ls'\n",
    "video4_url = \"https://www.youtube.com/watch?v=xjy-7ZCohG8\"\n",
    "\n",
    "yt = YouTube(video4_url, on_progress_callback=on_progress).streams[0]\n",
    "\n",
    "\n",
    "print(yt)\n",
    "yt.download('video4.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b9581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the timestamps and convert them from MM:SS formats to only seconds\n",
    "with open(\"video4_kill_timestamps.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [t[:-1] for t in lines]\n",
    "lines = [t.split(':') for t in lines]\n",
    "lines = [int(t[0])*60+int(t[1]) for t in lines]\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy as mp\n",
    "\n",
    "#load the downloaded video\n",
    "video = mp.VideoFileClip('video4.mp4')\n",
    "\n",
    "#clip out the t-2 to t+1 section for each timestamp t, and save it\n",
    "for t in lines:\n",
    "    t = int(t)\n",
    "    subclip = video[t-2:t+1]\n",
    "    subclip.write_videofile(f\"data/video4_{t-2}_{t+1}.mp4\", codec='libx264')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0e515",
   "metadata": {},
   "source": [
    "#### function for loading the clips as frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d8487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_as_frames(path:str):\n",
    "\n",
    "    video = cv2.VideoCapture(path)\n",
    "    success, img = video.read()\n",
    "\n",
    "    \n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frames = torch.zeros((frame_count, *img.shape))\n",
    "\n",
    "    i= 0\n",
    "    while success:\n",
    "\n",
    "        frames[i] = torch.Tensor(img)\n",
    "        i+=1\n",
    "        success, img = video.read()\n",
    "\n",
    "    \n",
    "    frames = torch.permute(frames, (0, 3, 1, 2))\n",
    "    \n",
    "    return frames\n",
    "\n",
    "    # torch.Tensor(frames[0])\n",
    "    # print(frames.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdcb6c3",
   "metadata": {},
   "source": [
    "### 2. The architechture(or pipeline, or whatver)\n",
    "\n",
    "* Firstly, each clip is loaded, then it is batched and a feature representation is obtained for each individual frame. A pretrained model, (such as a resnet) is used for this, while optionally unfreezing the last layer \n",
    "* The features are then passed sequentially into an LSTM one after another, a single vector is finally recieved, encapsulating the sequential form of the data.\n",
    "* now it is passed into a (Variational) autoencoder, and trained. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39423729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class myResnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.transforms = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
    "\n",
    "        self.model.fc = nn.Identity()\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        return self.transforms(X)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "class myLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        out, (hn, cn) = self.lstm(X)\n",
    "\n",
    "        #out.shape = (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        #take the last element from the sequence\n",
    "        if out.ndim == 3: #batched\n",
    "            out = out[:, -1, :] \n",
    "        elif out.ndim == 2: #unbatched\n",
    "            out = out[-1, :]\n",
    "\n",
    "\n",
    "        return out\n",
    "    \n",
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, latent_size, hidden_dims:list=[256, 128]):\n",
    "        \n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        hidden_dims.append(latent_size)\n",
    "        hidden_dims.insert(0, input_size)\n",
    "\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        layers = []\n",
    "        hidden_dims.reverse()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.encoder(X)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "    \n",
    "    def get_loss(self, input, output):\n",
    "        return F.mse_loss(input, output)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dims=[256, 128]):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        k=input_dim\n",
    "\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(k, dim))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "\n",
    "            k = dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.fc_mu = nn.Linear(k, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(k, latent_dim)\n",
    "\n",
    "        layers = []\n",
    "        hidden_dims.reverse()\n",
    "        k = latent_dim\n",
    "\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(k, dim))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            k = dim\n",
    "        \n",
    "        layers.append(nn.Linear(k, input_dim))\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        out = self.encoder(X)\n",
    "\n",
    "        #get mean and log(var)\n",
    "        mean = self.fc_mu(out)\n",
    "        log_var = self.fc_log_var(out)\n",
    "\n",
    "        #reparametrization\n",
    "        epsilon = torch.randn_like(mean)\n",
    "        z = mean + epsilon*log_var\n",
    "\n",
    "        #decode\n",
    "        X_recon = self.decoder(z)\n",
    "\n",
    "        return X_recon, mean, log_var\n",
    "    \n",
    "    def get_loss(self, X, target):\n",
    "\n",
    "        X_recon, mean, log_var = target\n",
    "\n",
    "        normal_loss = F.mse_loss(X, X_recon, reduce='sum') / X.shape[0]\n",
    "\n",
    "        kld_loss = -0.5 * torch.mean(1+log_var-mean**2-log_var.exp()) / X.shape[0]\n",
    "\n",
    "        return kld_loss + normal_loss\n",
    "\n",
    "\n",
    "class myGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        out, h_n = self.gru(X)\n",
    "\n",
    "        #take last element from the sequence. \n",
    "        #out.shape = (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        if out.ndim == 3: #batched\n",
    "            out = out[:, -1, :] \n",
    "        elif out.ndim == 2: #unbatched\n",
    "            out = out[-1, :]\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ## model for getting feature encodings\n",
    "        self.cnn_model = myResnet()\n",
    "        \n",
    "        ## LSTM (can also use attention based here if needed)\n",
    "        \n",
    "\n",
    "        self.lstm = myGRU(2048, 1024, 1)\n",
    "        \n",
    "        ## VAE\n",
    "\n",
    "        self.ae = VAE(1024, 128, [256, 128])\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        # assumes that X is an input of shape (seq_length, channels, height, width)\n",
    "\n",
    "        encodings = self.cnn_model(X)\n",
    "        #encoding has shape (seq_length, 2048), note that this is unbatched\n",
    "\n",
    "        lstm_encoding = self.lstm(encodings)\n",
    "        #this output has shape (1024, )\n",
    "\n",
    "        out = self.ae(lstm_encoding)\n",
    "     \n",
    "        return lstm_encoding, out #both are necessary for loss\n",
    "    \n",
    "\n",
    "    def preprocess(self, X):\n",
    "        return self.cnn_model.preprocess(X)\n",
    "    \n",
    "    def get_loss(self, lstm_encoding, output):\n",
    "\n",
    "        return self.ae.get_loss(lstm_encoding, output)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7603c328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10084352"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in MyModel().parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eebc84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_test_clips.json\") as f:\n",
    "    train_test_clips = json.load(f)\n",
    "\n",
    "train_clips = train_test_clips['train']\n",
    "test_clips = train_test_clips['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3c7b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357571b",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "iteration_losses = []\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(7):\n",
    "    \n",
    "    total_loss = 0.\n",
    "    start = time.time()\n",
    "\n",
    "    \n",
    "    random.shuffle(train_clips)\n",
    "\n",
    "    iteration = 0\n",
    "    for clip_path in train_clips:\n",
    "        iteration += 1\n",
    "        clip = load_video_as_frames(clip_path)\n",
    "\n",
    "        # print(clip.shape)\n",
    "        clip = model.preprocess(clip).to(device)\n",
    "\n",
    "        ae_in, ae_out = model(clip)\n",
    "        loss = model.get_loss(ae_in, ae_out)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        if (iteration+1)%10==0:\n",
    "            print(f\"loss for iteration{iteration+1} = {loss.item()}\")\n",
    "            iteration_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    epoch_losses.append(total_loss/len(train_clips))\n",
    "    print(f\"Epoch[{epoch}], avg_loss={total_loss/len(train_clips)}, time-taken={time.time()-start}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01523d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mymodel_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd54f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.781089544296265"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "# for clip_path in train_clips:\n",
    "#         # print(clip_path)\n",
    "#         clip = load_video_as_frames(\"./data/\"+clip_path)\n",
    "\n",
    "#         clip = model.preprocess(clip).to(device)\n",
    "\n",
    "clip = load_video_as_frames(\"./data/video3_467_470.mp4\").to(device)\n",
    "clip = model.preprocess(clip).to(device)[0:]\n",
    "print(clip.shape)\n",
    "ae_in, ae_out = model(clip)\n",
    "# print(ae_in.shape)\n",
    "# print(ae_out.shape)\n",
    "\n",
    "\n",
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc747c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for clip_path in test_clips:\n",
    "        print(clip_path)\n",
    "        clip = load_video_as_frames(\"./data/\"+clip_path)\n",
    "        clip = model.preprocess(clip).to(device)\n",
    "\n",
    "        ae_in, ae_out = model(clip)\n",
    "        loss = model.get_loss(ae_in, ae_out)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    print(losses)\n",
    "\n",
    "    print(sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aimbot_clips = os.listdir(\"./aimbot_clips\")\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for clip_path in aimbot_clips:\n",
    "        print(clip_path, end=\" \")\n",
    "        clip = load_video_as_frames(\"./aimbot_clips/\"+clip_path)\n",
    "        clip = model.preprocess(clip).to(device)\n",
    "\n",
    "        ae_in, ae_out = model(clip)\n",
    "        loss = model.get_loss(ae_in, ae_out)\n",
    "\n",
    "        print(loss.item())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # print(losses)\n",
    "\n",
    "    print(sum(losses)/len(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aimbot_clips = os.listdir(\"./aimbot_clips\")\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for clip_path in aimbot_clips:\n",
    "        print(clip_path, end=\" \")\n",
    "        clip = load_video_as_frames(\"./aimbot_clips/\"+clip_path)\n",
    "        clip = model.preprocess(clip).to(device)\n",
    "\n",
    "        ae_in, ae_out = model(clip)\n",
    "        loss = model.get_loss(ae_in, ae_out)\n",
    "\n",
    "        print(loss.item())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # print(losses)\n",
    "\n",
    "    print(sum(losses)/len(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb787f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    losses=[]\n",
    "\n",
    "    for t in lines:\n",
    "        t = int(t)\n",
    "        clip_path = f\"data/video4_{t-2}_{t+1}.mp4\"\n",
    "        clip = load_video_as_frames(clip_path)\n",
    "        clip = model.preprocess(clip).to(device)\n",
    "\n",
    "        ae_in, ae_out = model(clip)\n",
    "        loss = model.get_loss(ae_in, ae_out)\n",
    "\n",
    "        print(clip_path, \" \", loss.item())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361510a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
